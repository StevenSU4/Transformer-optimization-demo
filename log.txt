Warnings:
D:\Anaconda\envs\torch\lib\site-packages\torch\nn\modules\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}")
D:\Anaconda\envs\torch\lib\site-packages\torch\nn\functional.py:5504: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:455.)
  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)

Results:
Training with SGD
Epoch 1/5: Train Loss: 0.0212, Train Acc: 0.9984, Test Loss: 0.0018, Test Acc: 1.0000
Epoch 2/5: Train Loss: 0.0012, Train Acc: 1.0000, Test Loss: 0.0008, Test Acc: 1.0000
Epoch 3/5: Train Loss: 0.0007, Train Acc: 1.0000, Test Loss: 0.0005, Test Acc: 1.0000
Epoch 4/5: Train Loss: 0.0004, Train Acc: 1.0000, Test Loss: 0.0003, Test Acc: 1.0000
Epoch 5/5: Train Loss: 0.0003, Train Acc: 1.0000, Test Loss: 0.0003, Test Acc: 1.0000
Training with Adam
Epoch 1/5: Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0000, Test Acc: 1.0000
Epoch 2/5: Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0000, Test Acc: 1.0000
Epoch 3/5: Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0000, Test Acc: 1.0000
Epoch 4/5: Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0000, Test Acc: 1.0000
Epoch 5/5: Train Loss: 0.0000, Train Acc: 1.0000, Test Loss: 0.0000, Test Acc: 1.0000